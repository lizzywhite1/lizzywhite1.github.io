---
title: 'A deep dive into Linear Regression'
date: 2025-07-25
permalink: /posts/2025/07/linear-regression/
tags:
  - regression
  - basics
---
# Motivation
Brilliant another blog post on linear regression, how creative (*eye roll*). Why would I make this post (I hear no one ask)? 
Well having dabbled in a few ML/Statistics projects, I have begun to realise how important it is to have a very 
**strong foundational understanding** of core concepts, before trying to undertake more complex ones. 
So I put my ego and my desire to dive into all the flashy ML models aside, and went to the first chapter of any book on statistical learning. 
Linear regression.  

I tasked myself with the challenge of building a linear regression model from scratch in Python (no scipy allowed!). 
"*This will be easy*" I thought. But as I started to read more into 
Linear Regression and began to build out the model, I realised how shallow my knowledge of Linear Regression actually was. 

In this post I will take you along my journey to "*re-learning*" linear regression from scratch. I hope to present it in a 
way that may be different to how you were taught it at university (or at least how I was), and detail my intial pitfalls 
when attempting to build out my own mock Scipy linregress function. 

# Theory
The aim of this section is to give a succinct overview of everything (I beleive) a student, like myself, should know about 
linear regression. This is not a textbook; I will not be extrenely rigorous in definining notation, if you want to dive deeper, 
I reccommend checking out the resources at the end.
## What even is (linear) regression?
If I had to sum up linear regression in one sentence, I'd say this. 

> Linear regression encapsulates a class of functions that model the relationship between inputs and quantitative targets using a linear
> combination of fixed basis functions of the input. 

So what does all of this mean? 

In statistics and machine learning, *regression* refers to any task where
the goal is to predict a quantitive value (this could be a scalar or a vector) given one or more inputs. 
The term "*linear*" in linear regression, simply means our model is a linear function of the parameters, 
however **there is no requirement that we are linear with respect to our input vector**, $\textbf{x}$. More formally, we 
express our predicted output as a weighted sum of **basis functions** evaluated at the input; these models in the form $$f(\textbf{x};\textbf{w})=w_0+\sum ^{M-1} _{i=1} w_i \phi_i (\textbf{x})\tag{1}$$where
- $\textbf{x}=(x_1,...x_d)^T$ is our $d-$dimensional input vector containing the features in which the model learns to predict 
the target (or output). TODO: A two dimensional hyperplane FIGURE
- $\textbf{w}=(w_0,...,w_{M-1})^T$ is our $M-$ dimensional vector containing the model weights/parameters which are learnt to minimise 
some *error function* ie. the weights which help us minimise the distance between a true and predicted output.
- $\phi_i(\textbf{x})$ are our linear (or non-linear) basis functions of $\textbf{x}$. The span of these basis functions
makes up the finite dimensional subspace in which $f$ lives. 

**Key Takeaway 1:** Linear regression models are functions, $f$, which map from the input to output space such that $f$ lies
in the finite dimensional function space $\text{span}\{\phi_0,...,\phi_{M-1}\}$. 

A more compact way of expressing this, letting $\phi_0(\textbf{x})=1$ is

$$f(\textbf{x};\textbf{w})=\textbf{w}^T\bf{\phi}(\textbf{x})$$

This may seem foreign if youâ€™ve only ever encountered the standard linear regression model, which is written as

$$
f(\mathbf{x}; \mathbf{w}) = w_0 + \sum_{i=1}^{M-1} w_i x_i. \tag{2}
$$

However, this is equivalent to Equation (1) when our basis functions are simply the **linear basis functions** 
$\phi_j(\mathbf{x}) = x_j$. By extending this class of functions by using non-linear basis functions 
we can model more complex relationships between the inputs and the target variable. These basis functions could be polynomials,
trigonometric functions, interactions between features or a numeric coding of qualitative inputs.

**Key Takeaway 2**: Linear regression is so much more than just simple linear regression; the model must only be a linear function 
of the parameters, $\textbf{w}$. 

## Maximum Likelihood and Least Squares Framework
Here is where most textbooks would introduce the *residual sum of squares* ie. we want to find the parameters which minimise
the squared loss between the true output and the predicted output from our linear regression model. Suppose 
- We have labelled data $(x_1,y_1), ..., (x_n, y_n)$ where each input $x_i=(x_{i1},...,x_{id})^T$ is a $d-$dimensional vector and 
for the sake of simplicity, let $y$ be a real valued output.
- Define $\Phi\in \mathbb R^{n\times M}$, called the *design matrix*, where each row represents a new observation 
and that each column $j\in M$, represents $\phi_j(x_i)$ and $\phi_0 (x_i)=1$ for any $i\in n$.
- Lastly, we should specify that $\textbf{y}\in \mathbb R^n$ is a vector of target outputs for each input vector.

We would be interested in minimising the RSS, 

$$RSS(\textbf{w})=(\textbf{y}-\Phi \textbf{w})^T(\textbf{y}-\Phi \textbf{w})$$ 


which is a quadratic function in M parameters. Thus, the optimal $\textbf{w}$ is one such that $\textbf{w}\in \text{arg min} _{\textbf{w}}
\sum ^n _{i=1}(y_i - f(\textbf{x}_i;\textbf{w}))^2$. This can easily be found by the derivative with respect to $\textbf{w}$ 
and solving for when this equals zero (ie. finding the minimum of $\text{RSS}(\textbf{w})$). The solution is given by $$
\hat{\textbf{w}} = (\Phi^T\Phi)^{-1}\Phi^T\textbf{y}.
$$This gives us that our predicted value of $\textbf{y}$ is given as 

$$\hat{\textbf{y}}=\Phi \hat{\textbf{w}}=\Phi 
(\Phi^T\Phi)^{-1}\Phi^T\textbf{y}.$$ 

$\Phi 
(\Phi^T\Phi)^{-1}\Phi^T$ is called the 'hat' or projection matrix.

### Geometric Interpretation 
This leads to a nice geometric interpretation of the least squares framework, seen in Figure 2. 

### Probabilistic Interpretation of Linear Regression & Maximum Likelihood Representation
This approach to estimating parameters has never fully satisfied me for various reasons. Namely, why do we square the loss 
and why does this squared loss result in the best possible estimation of coeffcients? To address this, we can look at a 
probabilistic interpretation of linear regression. 



### Caveats and Numerical Methods for Estimation




## Geometric Interpretation



# Building our Linear Regression Model
## Problems to be aware of 

## Implementing In Practise 

# Conclusion 

# Further Reading / Referneces 
