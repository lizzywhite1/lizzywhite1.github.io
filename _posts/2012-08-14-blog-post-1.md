---
title: 'A deep dive into Linear Regression'
date: 2025-07-25
permalink: /posts/2025/07/linear-regression/
tags:
  - regression
  - basics
---
# Motivation
Brilliant another blog post on linear regression, how creative! Why would I make this post (I hear no one ask)? 
Having now dabbled in a few large ML/Statistics projects during my undergrad, I have begun to realise how important it is to have a very 
**strong foundational understanding** of core concepts, before trying to undertake more complex ones. 
So I put my ego and my desire to dive into all the new flashy machine learning models aside, and went to the first chapter of any book on statistical learning. 
**Linear regression**.  

I tasked myself with the challenge of building a linear regression model from scratch in Python (no scipy allowed!). 
"*This will be easy*" I thought. But as I started to read more into 
Linear Regression and began to build out the model, I realised how shallow my knowledge of Linear Regression actually was. 

In this post I will take you along my journey to "*re-learning*" linear regression from scratch. I hope to present it in a 
way that may be different to how you were taught it at university (or at least how I was), and detail my intial pitfalls 
when attempting to build out my own mock Scipy _linregress_ function. 

# Theory
The aim of this section is to give a succinct overview of everything (I beleive) a student, like myself, should know about 
linear regression. This is not a textbook; I will not be extrenely rigorous in definining notation, if you want to dive deeper, 
I reccommend checking out the resources at the end.
## What even is (linear) regression?
If I had to sum up linear regression in one sentence, I'd say this. 

> Linear regression encapsulates a class of functions that model the relationship between inputs and quantitative targets 
> using a linear combination of functions of the input. 

So what does all of this mean? 

In statistics and machine learning, *regression* refers to any task where
the goal is to predict a quantitive value (this could be a scalar or a vector) given a corresponding set of inputs. 
The term "*linear*" in linear regression, simply means our model is a linear function of the parameters (ie. weights), 
however **there is no requirement that we are linear with respect to our input vector**, $$\textbf{x}$$. More formally, we 
express our predicted output as a weighted sum of **basis functions** evaluated at the input; these models in the form 

$$f(\textbf{x};\textbf{w})=w_0+\sum ^{M-1} _{i=1} w_i \phi_i (\textbf{x})\tag{1}$$

where
- $$\textbf{x}=(x_1,...x_d)^T$$ is our $$d$$-dimensional input vector containing the features in which the model learns to predict 
the target (or output). 
- $$\textbf{w}=(w_0,...,w_{M-1})^T$$ is our $$M$$-dimensional vector containing the model weights which are learnt to minimise 
some *error function* ie. the weights which help us minimise the distance between a true and predicted output.
- $$\phi_i(\textbf{x})$$ are our basis functions of $$\textbf{x}$$. 

>**Key Takeaway 1:** Linear regression models are functions, $$f$$, which map from the input to output space such that $$f$$ lies
>in the finite dimensional function space $$\text{span}\{\phi_0,...,\phi_{M-1}\}$$. 

A more compact way of expressing this, letting $$\phi_0(\textbf{x})=1$$ is

$$f(\textbf{x};\textbf{w})=\textbf{w}^T\bf{\phi}(\textbf{x})$$

This may seem foreign if you’ve only ever encountered the standard linear regression model, which is written as

$$f(\mathbf{x}; \mathbf{w}) = w_0 + \sum_{i=1}^{M-1} w_i x_i. \tag{2}$$

However, Equation (2) is equivalent to Equation (1) when our basis functions are simply the **linear basis functions** 
$$\phi_j(\mathbf{x}) = x_j$$. 

By extending this class of functions by using non-linear basis functions we can model more complex relationships between 
the inputs and the target variable. These basis functions could be polynomials, trigonometric functions, interactions 
between features or a numeric coding of qualitative inputs.

>**Key Takeaway 2**: Linear regression is so much more than just simple linear regression; the model must only be a linear function 
>of the parameters, $$\textbf{w}$$. 

## Maximum Likelihood and Least Squares Framework
Here is where most textbooks would introduce the *residual sum of squares* ie. we want to find the parameters which minimise
the squared loss between the true output and the predicted output from our linear regression model. Suppose 
- We have labelled data $$(\textbf{x}_1,y_1), ..., (\textbf{x}_n, y_n)$$ where each input $$\textbf{x}_i=(x_{i1},...,x_{id})^T$$ is a $$d-$$dimensional vector and 
for the sake of simplicity, let $$y_i$$ be a real valued output.
- Define $$\Phi\in \mathbb R^{n\times M}$$, called the *design matrix*, where each row represents a new observation 
and that each column $$j\in M$$, represents $$\phi_j(x_i)$$ and $$\phi_0 (x_i)=1$$ for any $$i\in n$$.

$$\Phi = \begin{bmatrix}\end{bmatrix}$$

- Lastly, we should specify that $$\textbf{y}\in \mathbb R^n$$ is a vector of target outputs for each input vector, where
the $n$-th element $$y_n$$ is the corresponding target for input $$\textbf{x}_n$$. 

We would be interested in minimising the RSS, 

$$RSS(\textbf{w})=(\textbf{y}-\Phi \textbf{w})^T(\textbf{y}-\Phi \textbf{w})$$ 

which is a quadratic function in M parameters. Thus, the optimal $$\textbf{w}$$ is one such that 

$$\textbf{w}\in \text{arg min} _{\textbf{w}}
\sum ^n _{i=1}(y_i - f(\textbf{x}_i;\textbf{w}))^2.$$ 

This can easily be found by the derivative with respect to $$\textbf{w}$$ 
and solving for when this equals zero (ie. finding the minimum of $$\text{RSS}(\textbf{w})$$). The solution is given by 

$$
\hat{\textbf{w}} = (\Phi^T\Phi)^{-1}\Phi^T\textbf{y}.
$$

This gives us that our predicted value of $$\textbf{y}$$ is given as 

$$\hat{\textbf{y}}=\Phi \hat{\textbf{w}}=\Phi 
(\Phi^T\Phi)^{-1}\Phi^T\textbf{y}.$$ 

$$\Phi (\Phi^T\Phi)^{-1}\Phi^T$$ is called the 'hat' or projection matrix. 

If you have taken some undergraduate statistics courses, you will be familiar with this explanation, however personally,
this explanation has never fully satisfied me. My main gripe with this explanation is, "_Why are we interested in minimising
the RSS in the first place? Why a squared loss and not an absolute or cubed loss?_". Usually, you'll get some generic response
back, such as, it's easier to compute the derivative for the squared loss, however I was never fully convinced by this rationale. 

I will now present two alternative approaches for finding a solution to the linear model. 

### Geometric Interpretation 
This leads to a nice geometric interpretation of the least squares solution. Consider an $$N$$-dimensional 
subspace (where $$N$$ is the number of data points). Our vector $$\textbf{y}$$ containing the true values is a vector in this 
space. Additionally, each column vector, $$\phi_j(\textbf{x})$$ for $$j\in M$$, of $$\Phi$$ will be a vector in this 
$N$-dimensional subspace. 

The first thing to note is that, assuming $$M<N$$, then our set of $$M$$ column vectors, $$\phi_j(\textbf{x})$$, will span a 
linear subspace, $$\mathcal S$$, of dimensionality $$M$$. To understand why, first let's review what a linear subspace is. 

Without getting too heavy in notation, a linear subspace is simply a [vector space](https://en.wikipedia.org/wiki/Vector_space) 
which is a subset of a larger vector space. As a linear subspace is itself a vector space, it must contain the zero vector 
and be closed under both vector addition and scalar multiplication. We define linear subspaces through a minimal generating 
set of [linearly independent](https://en.wikipedia.org/wiki/Linear_independence) vectors, known as **basis vectors**. This set 
is referred to as the basis of the (sub)space. 

To formalise this, we say that $$\mathcal{B}$$ is a basis for a vector (sub)space $$V$$ if every vector in our vector (sub)space 
$$\textbf{x} \in V$$ can be uniquely expressed as

$$
\textbf{x} = \sum_{i=1}^{k} \lambda_i \textbf{b}_i
$$

for some scalars $$\lambda_i \in \mathbb{R}$$ and basis vectors $$\textbf{b}_i \in \mathcal{B}$$. The dimension of a subspace 
is defined as the number of vectors in our basis. Figure 1 shows an example of what a two-dimensional linear subspace would 
look like in a three-dimensional vector space.

<div id="subspace-plot" style="height: 600px;"></div>

<!-- Load Plotly.js -->
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

<script>
var v1 = [1, 2, 1];
var v2 = [2, -1, 1];

// Create a grid of scalars
var s_vals = [];
for (var i = -2; i <= 2; i += 0.4) s_vals.push(i);
var t_vals = s_vals.slice();

var X = [], Y = [], Z = [];
for (var i = 0; i < s_vals.length; i++) {
    X[i] = [], Y[i] = [], Z[i] = [];
    for (var j = 0; j < t_vals.length; j++) {
        var s = s_vals[i], t = t_vals[j];
        X[i][j] = s * v1[0] + t * v2[0];
        Y[i][j] = s * v1[1] + t * v2[1];
        Z[i][j] = s * v1[2] + t * v2[2];
    }
}

// Surface trace for the plane
var plane = {
    type: 'surface',
    x: X,
    y: Y,
    z: Z,
    opacity: 0.7,
    colorscale: 'Blues',
    showscale: false
};

// Cone traces for basis vectors
function coneTrace(vec, color) {
    return {
        type: 'cone',
        x: [0], y: [0], z: [0],
        u: [vec[0]], v: [vec[1]], w: [vec[2]],
        colorscale: [[0, color], [1, color]],
        sizemode: "absolute",
        sizeref: 0.6,
        showscale: false
    };
}

var data = [
    plane,
    coneTrace(v1, 'red'),
    coneTrace(v2, 'green')
];

var layout = {
    title: '2D Subspace in ℝ³',
    scene: {
        xaxis: { range: [-5, 5] },
        yaxis: { range: [-5, 5] },
        zaxis: { range: [-5, 5] },
        aspectmode: 'cube'
    }
};

Plotly.newPlot('subspace-plot', data, layout);
</script>

Returning to our linear regression model, recall that the predicted outputs are given by

$$
\hat{\textbf{y}} = \Phi \textbf{w}.
$$

Each row of the design matrix $$\Phi \in \mathbb{R}^{N \times M}$$ corresponds to the $$M$$ basis functions evaluated at 
a particular input $$\textbf{x}_n$$, while each column corresponds to a single basis function $$\phi_j$$ evaluated across all the $$N$$ 
data points.

Now, consider two model outputs:

$$
\hat{\textbf{y}}_1 = \Phi \textbf{w}_1, \quad \hat{\textbf{y}}_2 = \Phi \textbf{w}_2.
$$

Then their sum is

$$
\hat{\textbf{y}}_1 + \hat{\textbf{y}}_2 = \Phi (\textbf{w}_1 + \textbf{w}_2),
$$

which is clearly still of the form $$\Phi \textbf{w}$$ and hence remains within the set of model outputs. Similarly, for any scalar $$\alpha \in \mathbb{R}$$,

$$
\alpha \hat{\textbf{y}} = \Phi (\alpha \textbf{w})
$$

is again a valid model output. This confirms that the set of all predicted outputs spans a linear subspace, which we denote by $$\mathcal{S} \subset \mathbb{R}^N$$.

If the $$M$$ basis functions are linearly independent (in practise this might not be the case; more on this later!), 
then the columns of $$\Phi$$ also form a linearly independent set, and thus span an $$M$$-dimensional subspace. 
In other words, the model outputs live within an $$M$$-dimensional linear subspace of $$\mathbb{R}^N$$ defined by the image of $$\Phi$$.

> Key Takeaway 3: The set of basis functions for our linear model forms an $$M$$ dimensional subspace within $$\mathbb R^N$$
> where all possible model outputs live.

Intuitively, the best model prediction $$\hat{\textbf{y}}$$ is the one that is closest to the true output vector $$\textbf{y}$$ 
in terms of [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). 

This occurs when the **residual vector**, which is the difference $$\textbf{y} - \hat{\textbf{y}}$$, is orthogonal to the 
subspace $$\mathcal{S}$$ of all possible model outputs. More formally, the best model output $$\hat{\textbf{y}}$$ is the 
**orthogonal projection** of $$\textbf{y}$$ onto the subspace $$\mathcal{S}$$. 

To understand why this is the case, we must answer the following question:

> Why is the vector in the subspace whose difference from our target vector is orthogonal to the subspace the one that minimises the distance to the target?


### Probabilistic Interpretation of Linear Regression & Maximum Likelihood Representation


### Caveats and Numerical Methods for Estimation


# Building our Linear Regression Model
## Problems to be aware of 

## Implementing In Practise 

# Conclusion 

# Further Reading / Referneces 
