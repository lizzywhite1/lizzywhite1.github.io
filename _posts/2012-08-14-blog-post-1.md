---
title: 'A deep dive into Linear Regression'
date: 2025-07-25
permalink: /posts/2025/07/linear-regression/
tags:
  - regression
  - basics
---
Motivation

**Brilliant another blog post on linear regression, how original!** 

So why make this post? Well, for starters linear regression is the bread and butter of supervised learning; fail here, and it will come back to bite you. Having a deep understanding of the fundamentals of machine learning is crucial when it comes to tackling harder concepts. 

When I started writing this, I thought it would only take a few days. But as I dug deeper, I realised how incomplete my understanding of linear regression was and how many introductory texts to linear regression often leave many details and connections ignored.

This post will unpack linear regression from three angles; analytic, geometric and probablistic, and aim to connect allthree approaches such that you have a holistic understanding of linear regression. I will overview popular numerical methods for finding the least squares estimator and see how they compare in practise on a real world dataset. 

# Theory
## Problem Setup
Suppose we have a dataset of observations the goal of linear regression is to:

> *Predict the value of the output variable given the value of the input(s).*

To formalise this:
- Each observation is a vector $x\in \mathcal X$, where $\mathcal X$ is the input space. For example our observation could lie in $\mathbb R^2$ meaning it contains two real valued features. 
- Each observation has a corresponding target/label, $y\in \mathcal Y$. For simplicity, we will assume that our target is a scalar ie. our target $y\in\mathbb R$
- Our dataset consists of the $n$ observations, $\{(x_i,y_i\}_{i=1}^n\}\in \mathcal X \times \mathcal Y$. 

A linear model for regression is of the form: 
$$
f({x};\theta)=\theta_0+\sum ^{d-1} _{i=1} \theta_i \phi_i ({x})\tag{1}
$$
where
- $\theta=(\theta_0,...,\theta_{d-1})$ is our $d$-dimensional vector containing the model weights.
- $\phi_i : \mathcal X \to \mathbb R$ are our basis functions of ${x}$ which map from the input space to a real valued scalar. 

By concatenating all $\phi_i$ into a vector, $\phi\in \mathbb R^{d}$, called the feature vector, and letting $\phi_0({x})=1$, we can express Equation 1 as
$$
f({x};\theta)= \theta \phi^T ({x})  =\begin{pmatrix}\theta _0 & \theta_1 & \dots & \theta_{d-1}\end{pmatrix}\begin{pmatrix}1 \\ \phi_1(x) \\ \vdots \\ \phi_{d-1}(x)\end{pmatrix}.
$$
This may seem foreign if you’ve only ever encountered the standard linear regression model, which is written as
$$
f({x}; \theta) = \theta_0 + \sum_{i=1}^{d-1} \theta_i x_i. \tag{2}
$$

However, Equation (2) is equivalent to Equation (1) when our basis functions are simply the linear basis functions,$\phi_j({x}) = x_j$ for $j=1,...,d-1$

By extending this class of functions by using non-linear basis functions we can model more complex relationships between the inputs and the target variable. These basis functions could be polynomials, trigonometric functions, interactions between features or a numeric coding of qualitative inputs.

> **Key Takeaway**: Linear regression is so much more than just '*drawing the best straight line*'; the model must only be a linear function of the parameters, $\theta$, not neccessarily a linear function of $\textbf{x}$. We can think of the model output as a linear transformation of the parameters. 

## Maximum Likelihood and Least Squares Framework
Many books will jump straight into "minimising the residual sum of squares (RSS)" to find the closed form OLS formula, but I will approach this from the more general optimisation perspective using empirical risk minimisation.

### Minimising the Empirical Risk
We are interested in choosing suitable weights, $\theta\in \mathbb R$, such that we minimise the difference between the true and predicted output. We can achieve this by specifying and minimising a given loss function,  $\ell : \mathcal Y\times \mathcal Y\to \mathbb R$. 

The loss function $\ell(y,z)$ quantifies the loss from predicting $z$ when the true value is $y$. There are various loss functions that could be chosen for regression tasks.  One common loss function is the **squared loss**, $\ell(y,z)=(y-z)^2$. 

We are interested in minimising the average error over the data, known as the empirical risk,$\hat{\mathcal R}$, with respect to our model weights $\theta$,
$$
\hat{\mathcal R}(f_{\theta}) = \frac{1}{n}\sum ^n _{i=1} \ell (y_i, f({x}_i;\theta)),
$$
where $\hat{\theta}\in \text{arg min}_{\theta\in \Theta} \hat{\mathcal R}(f_{\theta})$ is known as the least squares estimator.

### Least Squares Framework
In the context of our linear model for regression, we define $\Phi\in \mathbb R^{n\times d}$, as the *design matrix*, where each row is the feature vector for one observation. 
$$
\Phi = \begin{pmatrix}1 & ... & \phi_{d-1}(x_1) \\ ... & & ...\\ 1 & ... & \phi_{d-1}(x_n) \end{pmatrix}=\begin{pmatrix}1 & \phi_1(\textbf{x})&\dots &\phi_{d-1}(\textbf{x}) \end{pmatrix},
$$
where $\phi_0 (\textbf{x}_i)=1$ for any $i\in n$ and $\textbf{x}=(x_1,\dots, x_n)^T\in \mathbb R^n$ is our vector of input observations. 

We will let $\textbf{y}=(y_1,...,y_n)^T\in \mathbb R^n$ be the vector of target outputs for each input vector, where the $i$-th element $y_i$ is the corresponding target for input $\textbf{x}_i$. 

We are therefore interested in minimising the empirical risk using the squared loss, which we can write in matrix notation as 
$$
\hat{\mathcal R}(\textbf{w})=\frac{1}{n}||\textbf{y}-\Phi \theta||^2_2, \tag{3}
$$
where $\vert \vert \cdot \vert \vert_2^2$ is the squared $\ell_2$-norm. 

### Ordinary Least Squares (OLS)
If $\Phi\in \mathbb R^{n\times d}$ has full column rank, which means that the column vectors $\phi _i(\textbf{x})$ are linearly independent, then $\hat{\theta}$ is known as the ordinary least squares estimator. As $\Phi$ has full column rank this implies that the number of features, $d$, must be less than or equal to the number of data points,$n$, ie. $d\le n$. If this isn't immediately obvious to you, ask yourself the question "_What is the maximum number of linearly independent vectors 
we can have in an $n$-dimensional space?_".

Given $\Phi$ has full column rank, we now want to show that the OLS estimator in Equation 3 exists and has a unique representation. While, we could just bulldoze ourselves through this and just assume it exists by making some hand-wavey statements, that's not what this blog is about. We are learning from first principles, so let's get stuck in. 

To answer this, let me remind you of a theorem you may have come across in Real Analysis: 

> [**Extreme Value Theorem**](https://en.wikipedia.org/wiki/Extreme_value_theorem): A continuous real valued function $f$ 
> on a closed and bounded (ie. compact) interval $[a,b]$ must obtain both a maximum and minimum, at least once [^1].

First things first, if we expand Equation 3 out we get,

$$
\hat{\mathcal R}(\textbf{w})=\frac{1}{n}(\textbf{y}-\Phi \theta)^T(\textbf{y}-\Phi\theta).\tag{4}
$$

which is quadratic in $\theta\in \mathbb R^d$, hence it is continuous on $\mathbb R^d$.

If you have never taken a convex analysis/optimisation course (like myself) you may be asking yourself how we can use the extreme value theorem considering when we are working with an unbounded set, $\mathbb R^d$, to prove the existence of a minimiser.

In order to approach this, we can use a theorem which allows us to drop our assumption of the function being continuous on a bounded set by showing it is a **coercive** function. The theorem is as follows:

> If $f$ is a continuous real-valued function on a closed and unbounded set, then $f$ has a global minimiser. If the first derivatives of $f$ exist, then the global minimisers are among the critical points of $f$. 

First let's define what it means for a function to be **coercive**. 

> A continuous function on an unbounded set, $S\subset \mathbb R^d$ is **coercive** if [^2]
> $$ 
> \lim _{||x||\to \infty } f(x)=+\infty
> $$

To prove that $\hat{\mathcal R}$ is coercive in $\theta$ we must show that 
$$
\lim _{||\theta||\to \infty} \frac{1}{n}||\textbf{y}-\Phi \theta||^2_2=+\infty.
$$

Using Equation 4, this can be rewritten as

$$
\frac{1}{n}\lim _{||\theta||\to \infty} (\textbf{y}-\Phi \theta)^T(\textbf{y}-\Phi\theta)=\frac{1}{n}\lim_{||\theta||\to 
\\infty}(||\textbf{y}||^2_2-2\theta^T\Phi^T\textbf{y}+\theta^T\Phi^T \Phi \theta).\tag{5}
$$

We know that because $\Phi$ has full column rank then $M=\Phi^T\Phi$ is positive definite and hence has non-zero eigenvalues.  As $M$ is also symmetric, we can use the fact that the [Rayleigh Quotient](https://www.sjsu.edu/faculty/guangliang.chen/Math253S20/lec4RayleighQuotient.pdf) is bounded below by the smallest eigenvalue,
$$
\min _{\theta\in \mathbb R^d} \frac{\theta^T M \theta}{\theta ^T \theta}= \lambda _{\text{min}}>0,
$$
where $\lambda _{\text{min}}$ is the smallest eigenvalue. So we can bound the quadratic term from below by 

$$
\theta^TM\theta \ge \lambda _{\text{min}}||\theta||^2
$$

Additionally, using the Cauchy-Schwartz inequality, we can bound the $\theta$ term from above

$$
\vert\theta^T\Phi^T\textbf{y}|\le \vert\vert\theta \vert \vert \vert\vert\Phi^T\textbf{y}\vert\vert.
$$

Combining this with Equation 5 we get,
$$
\frac{1}{n}(\vert\vert\textbf{y}\vert\vert^2_2-2\theta^T\Phi^T\textbf{y}+\theta^T\Phi^T \Phi \theta)\ge \frac{1}{n}
(\vert\vert\textbf{y}\vert\vert^2_2-2\vert\vert\theta \vert \vert \vert\vert\Phi^T\textbf{y}\vert\vert+\lambda_{\text{min}}\vert
\vert\theta\vert\vert^2),\tag{6}
$$

we see the lower bound of Equation 6 tends to $\infty$ as $\theta\to\infty$ and so 
$$
\lim _{||\theta||\to \infty} \frac{1}{n}||\textbf{y}-\Phi \theta||^2_2=+\infty.
$$
As our function is continuous and coercive on a closed and unbounded set we can conclude that a global minimiser exists. As it is differentiable we know the global minimiser(s) lie in the critical point of the function.

To find the minimiser(s) we differentiate Equation 4 which gives,
$$ 
\hat{\mathcal R}'(\theta)= \frac{2}{n}(\Phi ^T \Phi \theta -\Phi ^T \textbf{y}).
$$
Solving $\hat{\mathcal R}'(\theta)=0$ gives the solution
$$ 
\hat{\theta}= (\Phi^T\Phi)^{-1}\Phi ^T\textbf{y} \tag{7}
$$

which is **unique** as the second derivative of $\hat{\mathcal R}$, known as the Hessian,
$$
\hat{\mathcal R}''(\theta)= \frac{2}{n}\Phi^T\Phi =\frac{2}{n}M
$$
must be non-zero, as $M$ is positive definite. 

### Geometric Interpretation of Linear Regression
A more intuitive approach to finding the closed form solution in Equation 7, is to find it geometrically.

Consider an $n$-dimensional vector space. Our vector $\textbf{y}\in \mathbb R^n$ containing the true values, is a vector in this space. Additionally, each column vector, $\phi_j(\textbf{x})$ for $j\in M$, of $\Phi$ will be a vector in this 

The first thing to note is that, given $\Phi$ has full column rank, then our set of $d$ column vectors, $\phi_j({x})$, will span a linear subspace, $\mathcal S$, of dimensionality $d$. These linearly independent column vectors will define the basis for this subspace, $\mathcal S$, which is the column space of $\Phi$.

> Key Takeaway: The set of basis functions for our linear model forms an $d$ dimensional subspace within $\mathbb R^N$ where all possible model outputs lie.

Intuitively, the best model predictions, $\hat{\textbf{y}}\in \mathbb R^n$, is the one that is closest to the true output vector $\textbf{y}$ in terms of [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). In order to minimise the distance, $\textbf{y}-\hat{\textbf{y}}$ we must find the orthogonal projection of $\textbf{y}$ onto $\mathcal S$, denoted $\text{Proj}_{\mathcal S}\textbf{y}$. 

The case with three data points, where the linear model is 

$$
y=\theta_0 + \theta_1 \phi _1({x}_i) \implies \textbf{y}=\Phi\theta=\begin{pmatrix} 1 & \phi_1({x}_1)
\\ 1 & \phi _1({x}_2)\\ 1 & \phi_1({x}_3)\end{pmatrix}\begin{pmatrix} \theta _0 \\ \theta_1\end{pmatrix}
$$

is illustrated in the interactive visualization below. 

{% include interactive_plot.html plot_file="assets/plots/orthogonal_projection_3d.html" width="800" height="600" caption="Interactive 3D visualization showing the orthogonal projection of vector y onto subspace S spanned by basis functions φ₀(x) = 1 and φ₁(x). The plot demonstrates how the least squares solution corresponds to finding the orthogonal projection." %}

To understand why this is the case, we must answer the following question:
1. Why is the orthogonal projection of $\textbf{y}$ onto $\mathcal S$ the closest vector in our subspace to $\textbf{y}$?

Let $\textbf{a}=\textbf{y} - \text{Proj}_{\mathcal S}(\textbf{y})$ be the residual vector, meaning it is orthogonal to every vector in $\mathcal S$. Let $\textbf{u}\in \mathcal S$ be an arbitrary vector in the subspace and let $\textbf{v}= \textbf{y}-\textbf{u}$. 

We want to show that the distance from the true outputs to its orthogonal projection onto $\mathcal S$ is shorter than the distance from any other arbitrary vector in the subspace. 

$$
||\textbf{a}||\le ||\textbf{v}||.
$$

Letting $\textbf{b}=\text{Proj}_{\mathcal S}(\textbf{y})-\textbf{u}\in S$, we know that $\textbf{a}\perp\textbf{b}$ so it follows that 
$$
||\textbf{v}||^2=||\textbf{a}||^2+||\textbf{b}||^2\ge ||\textbf{a}||^2,
$$

since $\vert\vert\textbf{b}\vert\vert^2\ge 0$. 

2. How do we find the orthogonal projection of $\textbf{y}$?

As the residual vector, is such that it is orthogonal to every vector in the subspace, we know that 
$\langle \phi_i(\textbf{x}), \textbf{y}-\hat{\textbf{y}}\rangle=0$. So, for any column vector $\phi_j({x})$
$$
(\textbf{y}-\hat{\textbf{y}})\cdot \phi_j({x})=0\implies \phi_j({x})\cdot\hat{\textbf{y}}=\textbf{y}\cdot\phi_j(
\textbf{x}).
$$

We can write this as,
$$
\phi_j(\textbf{x})\cdot (\theta_0 \phi_0(\textbf{x}) + ... + \theta_{d-1}\phi_{d-1}(\textbf{x}))=\phi_j(\textbf{x})\cdot \textbf{y},
$$

and expand to get,
$$
\theta_0\ \phi_j(\textbf{x})\cdot \phi_0(\textbf{x})+...+ \theta _{d-1}\phi_j(\textbf{x})\cdot \phi_{d-1}(\textbf{x})=\phi_j(\textbf{x})\cdot \textbf{y}.
$$

For column vectors $\phi_j(\textbf{x})$ from $j=0,...,d-1$ we write, 
$$
\begin{pmatrix} \phi_0^T(\textbf{x}) \phi _0(\textbf{x}) && ... && \phi_0^T(\textbf{x})\phi_{d-1}(\textbf{x})
\\ ... && && ...\\ \phi_{d-1}^T(\textbf{x}) \phi_0(\textbf{x}) && ... && \phi_{d-1}^T(\textbf{x})\phi_{d-1}(\textbf{x})\end{pmatrix}
\begin{pmatrix}\theta_0\\ \vdots \\ \theta_{d-1}\end{pmatrix} = \begin{pmatrix}\phi_0^T(\textbf{x})\textbf{y}\\\vdots \\\phi_{d-1}^T(\textbf{x})
\textbf{y}\end{pmatrix}
$$
$$
\implies \Phi^T\Phi \theta=\Phi\textbf{y}\implies \hat{\theta}=(\Phi ^T\Phi)^{-1}\Phi^T\textbf{y}.
$$

So we return to our closed form solution to least squares regression, 
$$
\hat{\textbf{y}}=\Phi(\Phi^T\Phi)^{-1}\Phi^T\textbf{y}
$$

where $\Phi(\Phi^T\Phi)^{-1}\Phi^T$ is our **projection** matrix!

> **Key Takeaway**: $\Phi(\Phi^T\Phi)^{-1}\Phi^T$ is simply the projection matrix which projects $$\textbf{y}$$ onto the subspace of model predictions, $\mathcal S$. The coefficients, $\theta$ which minimise the distance between the true and predicted output are those that define the projection of $\textbf{y}$ onto $\mathcal S$. 

### Probabilistic Interpretation of Linear Regression 
[Introduction]

We will assume that our inputs are fixed meaning the linear model outputs $f(x;\theta)$ are deterministic. We will again ensure that $\Phi$ is of full column rank. Lastly, we will make the assumption that the relationship between our inputs and targets can be modelled as 
$$
y_i=f({x}_i;\theta)+\epsilon_i
$$
where $\epsilon _i$ are independent Gaussian random variables with zero mean and fixed variance, $\sigma^2$, for $i\in \{1,...,n\}$. 

Here, the $\epsilon \in \mathbb R ^n$ is a vector to account for noise in measurements [^3]. We can see that $\mathbb E[y_i]=f({x}_i;\theta)$ and that $\text{Var}(y_i) = \sigma^2$ as $f(x_i;\theta)$ is deterministic.

Using the independence of $\epsilon _i$, we can express the likelihood as,
$$
p(\textbf{y}| \theta, \sigma^2) = \prod ^n_{i=1}\mathcal N(y_i|f({x}_i;\theta),\sigma^2) \tag{8}.
$$
Taking the logarithm of Equation 4, we obtain the log likelihood,
$$
\begin{align}
\ln p(\textbf{y}|\theta, \sigma^2) &= \sum^N_{i=1} \ln \mathcal N (y_i|\phi(\textbf{x}_i ;\theta),\sigma^2)
\\
&= \sum^N_{i=1} \ln \biggr(\frac{1}{\sqrt{2\pi \sigma^2}} \exp \biggr(- \frac{1}{2\sigma^2}(y_i-\phi(\textbf{x}_i)\theta^T)^2 \biggr)\biggr)
\\
&=-\frac{N}{2}\ln \biggr(2\pi\sigma^2 \biggr) -\frac{1}{2\sigma^2}||\textbf{y}-\Phi \theta||^2_2. \tag{9}
\end{align}
$$

Taking the derivative of Equation 9 with respect to $\theta$ and solving for zero, we find the maximum likelihood estimate is 
$$
\hat{\theta}=(\Phi^T\Phi)^{-1}\Phi^T \textbf{y},
$$

which is OLS estimator we found before when minimising the squared loss! 

> **Key Takeaway**: Assuming $\Phi$ is deterministic and that the relationship between the inputs and outputs can be modelled as $f(\textbf{x};\theta)$ with additive Gaussian noise, then the OLS estimator corresponds to the MLE.

_A quick note about the assumption of Gaussian noise:

- Anyone who has modelled real-world data using a linear regression model will know that the distribution of residuals is often not perfectly normal, so why do we make this bold assumption? A more useful question than asking "*are these assumptions true?*" , is "*are these assumptions plausible enough to make progress*" without distorting the results [^4] ?
- For a linear regression model, $\textbf{y}=\Phi\theta + \epsilon$ the only assumptions we place on $\epsilon$ are the Gauss Markov assumptions:
  1. $\mathbb E[\epsilon_i]=0$ meaning the errors are unbiased
  2. Distinct errors are uncorrelated ie. $\text{Cov}(\epsilon_i,\epsilon_j)=0$ 
  3. The errors have finite variance (homoscedastic)
- These assumptions guarantee that the OLS estimator is the best linear unbiased estimator (BLUE). These assumptions are the minimal conditions for our linear estimator to be unbiased and efficient (ie. have the smallest variance).
- By adding the additional assumption that the noise terms $\epsilon_i$ are Gaussian and i.i.d., the OLS estimator also becomes the maximum likelihood estimator. While this assumption is not essential, it makes inference analytically tractable (have a closed form solution which is easy to compute). Even if it does not strictly hold, the central limit theorem implies that, for large samples, the average of the error terms will tend toward a Gaussian distribution, making this assumption a reasonable approximation in many practical settings.  

### Numerical Methods for Estimation
Starting to step away from the theory, we shift are focus to how we find $$\hat \theta$$ numerically. Particularly, what 
happens as $$\Phi^T\Phi$$ becomes close to being singular? There are many ways to numerically compute the least squares estimator.

1. QR Decomposition
2. Gradient Descent 

# Building our Linear Regression Model
## Problems to be aware of 

## Implementing In Practise 



# Conclusion 

# Further Reading / Referneces 

##### Footnotes 
[^1]: I will not prove this here, because a) if you have taken any real analysis / calculus classes you would've come across this, if not a simple Google search would do b) the idea is rather intutitve and the proof doesn't add much aside from an appreciation of real analysis. 

[^2]: Taken from a [very great mini-lecture](https://www.youtube.com/watch?v=vWY6J7Eo8TM)

[^3]: This is often used interchangably with the term "_residual_", however, they are different. The residual refers to the difference between the model prediction and the true value whereas the error refers to difference between the observed value and the true data generating process. 

[4^]: Explained, beautifully in [CS229 Lecture 3 (Spring 2022)](https://www.youtube.com/watch?v=k_pDh_68K6c&list=PLoROMvodv4rNyWOpJg_Yh4NSqI4Z4vOYy&index=3)
